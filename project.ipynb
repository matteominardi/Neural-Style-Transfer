{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms , models \n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing VGG19 pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg19(pretrained=True).features\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If size is an int, smaller edge of the image will be matched to this number.\n",
    "\n",
    "Given mean: $(mean[1],...,mean[n])$ and std: $(std[1],..,std[n])$ for $n$ channels, this transform will normalize each channel of the input i.e.: $$output[channel] = (input[channel] - mean[channel]) / std[channel]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(300),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = Image.open(\"content.jpg\").convert(\"RGB\")\n",
    "content = transform(content).to(device)\n",
    "# print(\"COntent shape => \", content.shape)\n",
    "style = Image.open(\"style.jpg\").convert(\"RGB\")\n",
    "style = transform(style).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imcnvt(image):\n",
    "    x = image.to(\"cpu\").clone().detach().numpy().squeeze()\n",
    "    x = x.transpose(1,2,0)\n",
    "    x = x*np.array((0.5,0.5,0.5)) + np.array((0.5,0.5,0.5))\n",
    "    return np.clip(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "ax1.imshow(imcnvt(content),label = \"Content\")\n",
    "ax2.imshow(imcnvt(style),label = \"Style\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making sure the device is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting content and style features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the paper's guidelines, the content representation of the image is collected at the second convolution of the fourth layer, $conv4\\_2$, so it only uses a single layer of the network.\n",
    "On the other hand, the style representation is given by a number of layers of the CNN, hence why we need $conv1\\_1$, $conv2\\_1$, $conv3\\_1$, $conv4\\_1$, $conv5\\_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VGG19](https://www.researchgate.net/profile/Clifford-Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg \"VGG 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_activations(input,model):\n",
    "    layers = {\n",
    "    '0' : 'conv1_1',\n",
    "    '5' : 'conv2_1',\n",
    "    '10': 'conv3_1',\n",
    "    '19': 'conv4_1',\n",
    "    '21': 'conv4_2', # Content representation extracted from here\n",
    "    '28': 'conv5_1'\n",
    "    }\n",
    "    features = {}\n",
    "    x = input\n",
    "    x = x.unsqueeze(0)\n",
    "    for name,layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features = model_activations(content, model)\n",
    "style_features = model_activations(style, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Loss functions and computation of final image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image is encoded in each layer of the CNN by the filter responses to that image. <br>\n",
    "A layer with $N_l$ filters has $N_l$ feature maps, each of size $M_l$ (height times width of the feature maps). So the responses in a layer $l$ can be stored in a matrix $$F^l \\in R^{N_l \\times M_l}$$ where $F_{ij}^l$ is the activation of the $i$-th filter at position $j$ in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The style representation computes the correlations between different filter responses. These expectations are given by the Gram matrix, one for each layer, which is a square matrix where each element represents the inner product between the vectorised feature map $i$ and $j$ in that layer $l$. $$G_{ij}^l = \\sum_k F_{ik}^lF_{jk}^l$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(imgfeature):\n",
    "    _, d, h, w = imgfeature.size()\n",
    "    imgfeature = imgfeature.view(d, h*w)\n",
    "    gram_mat = torch.mm(imgfeature, imgfeature.t())\n",
    "    \n",
    "    return gram_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global loss function we minimise is: $$L_{total}(p,a,x) = \\alpha L_{content}(p,x) + \\beta L_{style}(a,x)$$ where the ratio $\\alpha / \\beta$ should be either $0.001$ or $0.0001$, $p$ is the original content image, $a$ is the original style image, and $x$ is the generated image proposal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_wt = 100\n",
    "style_wt = 1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define: $$L_{content}(p,x,l) = \\frac{1}{2} \\sum_{i,j}(F_{ij}^l - P_{ij}^l)^2$$ $$L_{style}(a,x) = \\sum_{l=0}^L \\omega_lE_l$$ Where all 5 selected layers have equal weight, and others are all set to zero, and $E_l$ is the contribution of each style layer to the total loss, and is defined as: $$ E_l = \\frac{1}{4N_l^2M_l^2} \\sum_{ij} (G_{ij}^l - A_{ij}^l)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_wt_meas = {\"conv1_1\" : 0.2, \n",
    "                \"conv2_1\" : 0.2,\n",
    "                \"conv3_1\" : 0.2,\n",
    "                \"conv4_1\" : 0.2,\n",
    "                \"conv5_1\" : 0.2}\n",
    "\n",
    "style_grams = {layer : gram_matrix(style_features[layer]) for layer in style_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([target],lr=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_after = 500\n",
    "epochs = 4000\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    target_features = model_activations(target,model)\n",
    "    content_loss = torch.mean((content_features['conv4_2']-target_features['conv4_2'])**2)\n",
    "\n",
    "    style_loss = 0\n",
    "    for layer in style_wt_meas:\n",
    "        style_gram = style_grams[layer]\n",
    "        target_gram = target_features[layer]\n",
    "        _, d, w, h = target_gram.shape\n",
    "        target_gram = gram_matrix(target_gram)\n",
    "\n",
    "        style_layer_loss = torch.mean((target_gram-style_gram)**2)/(4 * w * h)\n",
    "\n",
    "        style_loss += style_wt_meas[layer] * style_layer_loss \n",
    "    \n",
    "    total_loss = content_wt*content_loss + style_wt*style_loss \n",
    "    \n",
    "    if i % 10==0:       \n",
    "        print(\"epoch \",i,\" \", total_loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i%print_after == 0:\n",
    "        plt.imshow(imcnvt(target),label=\"Epoch \"+str(i))\n",
    "        plt.show()\n",
    "        plt.imsave(str(i)+'.png',imcnvt(target),format='png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
